/*
AUTOGENERATED-STYLE KERNEL (FULL, NO MISSING PARTS)
Target:
  LMUL=2 (Main blocks)
  M=16 (main)
  N=8 (main)
  cpu='zvl256b' (VLEN=256 bits)
  param_precision='float'
*/

#include <riscv_vector.h>

typedef long  BLASLONG;
typedef float FLOAT;

#ifndef CNAME
#define CNAME sgemm_kernel_16x8_zvl256b_lmul2_opt_unroll8
#endif

// Cross-compiler compatibility for unroll pragmas (Factor 8)
#if defined(__clang__)
    #define PRAGMA_UNROLL_8 _Pragma("clang loop unroll_count(8)")
#elif defined(__GNUC__)
    #define PRAGMA_UNROLL_8 _Pragma("GCC unroll 8")
#else
    #define PRAGMA_UNROLL_8
#endif

int CNAME(BLASLONG M, BLASLONG N, BLASLONG K,
          FLOAT alpha, FLOAT* A, FLOAT* B, FLOAT* C, BLASLONG ldc)
{
    BLASLONG m_top = 0;
    BLASLONG n_top = 0;
    size_t   gvl16 = 0;   // VL=16 for m2 on VLEN=256
    size_t   gvl   = 0;   // for tails

    if (M <= 0 || N <= 0) return 0;
    if (K <= 0) return 0;

    gvl16 = __riscv_vsetvl_e32m2(16);

    // =========================================================
    // -- MAIN PASS: N in blocks of 8
    // =========================================================
    for (BLASLONG j = 0; j < N/8; j += 1) {
        m_top = 0;

        // -------------------------
        // Main M loop: M/16 blocks using LMUL=2
        // -------------------------
        for (BLASLONG i = 0; i < M/16; i += 1) {
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            // k = 0
            float B0 = B[bi+0], B1 = B[bi+1], B2 = B[bi+2], B3 = B[bi+3];
            float B4 = B[bi+4], B5 = B[bi+5], B6 = B[bi+6], B7 = B[bi+7];
            bi += 8;

            // Load A: 16 rows as ONE vector of 16 (LMUL=2)
            vfloat32m2_t A0 = __riscv_vle32_v_f32m2(&A[ai], gvl16);
            ai += 16;

            // Accumulators for 8 columns, 1 vector each (16 rows)
            vfloat32m2_t r0 = __riscv_vfmul_vf_f32m2(A0, B0, gvl16);
            vfloat32m2_t r1 = __riscv_vfmul_vf_f32m2(A0, B1, gvl16);
            vfloat32m2_t r2 = __riscv_vfmul_vf_f32m2(A0, B2, gvl16);
            vfloat32m2_t r3 = __riscv_vfmul_vf_f32m2(A0, B3, gvl16);
            vfloat32m2_t r4 = __riscv_vfmul_vf_f32m2(A0, B4, gvl16);
            vfloat32m2_t r5 = __riscv_vfmul_vf_f32m2(A0, B5, gvl16);
            vfloat32m2_t r6 = __riscv_vfmul_vf_f32m2(A0, B6, gvl16);
            vfloat32m2_t r7 = __riscv_vfmul_vf_f32m2(A0, B7, gvl16);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                B4 = B[bi+4]; B5 = B[bi+5]; B6 = B[bi+6]; B7 = B[bi+7];
                bi += 8;

                A0 = __riscv_vle32_v_f32m2(&A[ai], gvl16);
                ai += 16;

                r0 = __riscv_vfmacc_vf_f32m2(r0, B0, A0, gvl16);
                r1 = __riscv_vfmacc_vf_f32m2(r1, B1, A0, gvl16);
                r2 = __riscv_vfmacc_vf_f32m2(r2, B2, A0, gvl16);
                r3 = __riscv_vfmacc_vf_f32m2(r3, B3, A0, gvl16);
                r4 = __riscv_vfmacc_vf_f32m2(r4, B4, A0, gvl16);
                r5 = __riscv_vfmacc_vf_f32m2(r5, B5, A0, gvl16);
                r6 = __riscv_vfmacc_vf_f32m2(r6, B6, A0, gvl16);
                r7 = __riscv_vfmacc_vf_f32m2(r7, B7, A0, gvl16);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m2_t c0 = __riscv_vle32_v_f32m2(&C[ci + 0*ldc], gvl16);
            vfloat32m2_t c1 = __riscv_vle32_v_f32m2(&C[ci + 1*ldc], gvl16);
            vfloat32m2_t c2 = __riscv_vle32_v_f32m2(&C[ci + 2*ldc], gvl16);
            vfloat32m2_t c3 = __riscv_vle32_v_f32m2(&C[ci + 3*ldc], gvl16);
            vfloat32m2_t c4 = __riscv_vle32_v_f32m2(&C[ci + 4*ldc], gvl16);
            vfloat32m2_t c5 = __riscv_vle32_v_f32m2(&C[ci + 5*ldc], gvl16);
            vfloat32m2_t c6 = __riscv_vle32_v_f32m2(&C[ci + 6*ldc], gvl16);
            vfloat32m2_t c7 = __riscv_vle32_v_f32m2(&C[ci + 7*ldc], gvl16);

            c0 = __riscv_vfmacc_vf_f32m2(c0, alpha, r0, gvl16);
            c1 = __riscv_vfmacc_vf_f32m2(c1, alpha, r1, gvl16);
            c2 = __riscv_vfmacc_vf_f32m2(c2, alpha, r2, gvl16);
            c3 = __riscv_vfmacc_vf_f32m2(c3, alpha, r3, gvl16);
            c4 = __riscv_vfmacc_vf_f32m2(c4, alpha, r4, gvl16);
            c5 = __riscv_vfmacc_vf_f32m2(c5, alpha, r5, gvl16);
            c6 = __riscv_vfmacc_vf_f32m2(c6, alpha, r6, gvl16);
            c7 = __riscv_vfmacc_vf_f32m2(c7, alpha, r7, gvl16);

            __riscv_vse32_v_f32m2(&C[ci + 0*ldc], c0, gvl16);
            __riscv_vse32_v_f32m2(&C[ci + 1*ldc], c1, gvl16);
            __riscv_vse32_v_f32m2(&C[ci + 2*ldc], c2, gvl16);
            __riscv_vse32_v_f32m2(&C[ci + 3*ldc], c3, gvl16);
            __riscv_vse32_v_f32m2(&C[ci + 4*ldc], c4, gvl16);
            __riscv_vse32_v_f32m2(&C[ci + 5*ldc], c5, gvl16);
            __riscv_vse32_v_f32m2(&C[ci + 6*ldc], c6, gvl16);
            __riscv_vse32_v_f32m2(&C[ci + 7*ldc], c7, gvl16);

            m_top += 16;
        }

        if (M & 8) {
            gvl = __riscv_vsetvl_e32m1(8); // Fallback to m1 for tails
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1], B2 = B[bi+2], B3 = B[bi+3];
            float B4 = B[bi+4], B5 = B[bi+5], B6 = B[bi+6], B7 = B[bi+7];
            bi += 8;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 8;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);
            vfloat32m1_t r2 = __riscv_vfmul_vf_f32m1(Av, B2, gvl);
            vfloat32m1_t r3 = __riscv_vfmul_vf_f32m1(Av, B3, gvl);
            vfloat32m1_t r4 = __riscv_vfmul_vf_f32m1(Av, B4, gvl);
            vfloat32m1_t r5 = __riscv_vfmul_vf_f32m1(Av, B5, gvl);
            vfloat32m1_t r6 = __riscv_vfmul_vf_f32m1(Av, B6, gvl);
            vfloat32m1_t r7 = __riscv_vfmul_vf_f32m1(Av, B7, gvl);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                B4 = B[bi+4]; B5 = B[bi+5]; B6 = B[bi+6]; B7 = B[bi+7];
                bi += 8;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                r4 = __riscv_vfmacc_vf_f32m1(r4, B4, Av, gvl);
                r5 = __riscv_vfmacc_vf_f32m1(r5, B5, Av, gvl);
                r6 = __riscv_vfmacc_vf_f32m1(r6, B6, Av, gvl);
                r7 = __riscv_vfmacc_vf_f32m1(r7, B7, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);
            vfloat32m1_t c2 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc], gvl);
            vfloat32m1_t c3 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc], gvl);
            vfloat32m1_t c4 = __riscv_vle32_v_f32m1(&C[ci + 4*ldc], gvl);
            vfloat32m1_t c5 = __riscv_vle32_v_f32m1(&C[ci + 5*ldc], gvl);
            vfloat32m1_t c6 = __riscv_vle32_v_f32m1(&C[ci + 6*ldc], gvl);
            vfloat32m1_t c7 = __riscv_vle32_v_f32m1(&C[ci + 7*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);
            c2 = __riscv_vfmacc_vf_f32m1(c2, alpha, r2, gvl);
            c3 = __riscv_vfmacc_vf_f32m1(c3, alpha, r3, gvl);
            c4 = __riscv_vfmacc_vf_f32m1(c4, alpha, r4, gvl);
            c5 = __riscv_vfmacc_vf_f32m1(c5, alpha, r5, gvl);
            c6 = __riscv_vfmacc_vf_f32m1(c6, alpha, r6, gvl);
            c7 = __riscv_vfmacc_vf_f32m1(c7, alpha, r7, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc], c2, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc], c3, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 4*ldc], c4, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 5*ldc], c5, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 6*ldc], c6, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 7*ldc], c7, gvl);
            m_top += 8;
        }

        if (M & 4) {
            gvl = __riscv_vsetvl_e32m1(4);
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1], B2 = B[bi+2], B3 = B[bi+3];
            float B4 = B[bi+4], B5 = B[bi+5], B6 = B[bi+6], B7 = B[bi+7];
            bi += 8;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 4;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);
            vfloat32m1_t r2 = __riscv_vfmul_vf_f32m1(Av, B2, gvl);
            vfloat32m1_t r3 = __riscv_vfmul_vf_f32m1(Av, B3, gvl);
            vfloat32m1_t r4 = __riscv_vfmul_vf_f32m1(Av, B4, gvl);
            vfloat32m1_t r5 = __riscv_vfmul_vf_f32m1(Av, B5, gvl);
            vfloat32m1_t r6 = __riscv_vfmul_vf_f32m1(Av, B6, gvl);
            vfloat32m1_t r7 = __riscv_vfmul_vf_f32m1(Av, B7, gvl);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                B4 = B[bi+4]; B5 = B[bi+5]; B6 = B[bi+6]; B7 = B[bi+7];
                bi += 8;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                r4 = __riscv_vfmacc_vf_f32m1(r4, B4, Av, gvl);
                r5 = __riscv_vfmacc_vf_f32m1(r5, B5, Av, gvl);
                r6 = __riscv_vfmacc_vf_f32m1(r6, B6, Av, gvl);
                r7 = __riscv_vfmacc_vf_f32m1(r7, B7, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);
            vfloat32m1_t c2 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc], gvl);
            vfloat32m1_t c3 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc], gvl);
            vfloat32m1_t c4 = __riscv_vle32_v_f32m1(&C[ci + 4*ldc], gvl);
            vfloat32m1_t c5 = __riscv_vle32_v_f32m1(&C[ci + 5*ldc], gvl);
            vfloat32m1_t c6 = __riscv_vle32_v_f32m1(&C[ci + 6*ldc], gvl);
            vfloat32m1_t c7 = __riscv_vle32_v_f32m1(&C[ci + 7*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);
            c2 = __riscv_vfmacc_vf_f32m1(c2, alpha, r2, gvl);
            c3 = __riscv_vfmacc_vf_f32m1(c3, alpha, r3, gvl);
            c4 = __riscv_vfmacc_vf_f32m1(c4, alpha, r4, gvl);
            c5 = __riscv_vfmacc_vf_f32m1(c5, alpha, r5, gvl);
            c6 = __riscv_vfmacc_vf_f32m1(c6, alpha, r6, gvl);
            c7 = __riscv_vfmacc_vf_f32m1(c7, alpha, r7, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc], c2, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc], c3, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 4*ldc], c4, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 5*ldc], c5, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 6*ldc], c6, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 7*ldc], c7, gvl);
            m_top += 4;
        }

        if (M & 2) {
            float r00=0,r01=0, r10=0,r11=0, r20=0,r21=0, r30=0,r31=0;
            float r40=0,r41=0, r50=0,r51=0, r60=0,r61=0, r70=0,r71=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            PRAGMA_UNROLL_8
            for (BLASLONG k=0; k<K; k++) {
                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                r20 += a0 * B[bi+2]; r21 += a1 * B[bi+2];
                r30 += a0 * B[bi+3]; r31 += a1 * B[bi+3];
                r40 += a0 * B[bi+4]; r41 += a1 * B[bi+4];
                r50 += a0 * B[bi+5]; r51 += a1 * B[bi+5];
                r60 += a0 * B[bi+6]; r61 += a1 * B[bi+6];
                r70 += a0 * B[bi+7]; r71 += a1 * B[bi+7];
                ai += 2;
                bi += 8;
            }

            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r00;  C[ci + 0*ldc + 1] += alpha * r01;
            C[ci + 1*ldc + 0] += alpha * r10;  C[ci + 1*ldc + 1] += alpha * r11;
            C[ci + 2*ldc + 0] += alpha * r20;  C[ci + 2*ldc + 1] += alpha * r21;
            C[ci + 3*ldc + 0] += alpha * r30;  C[ci + 3*ldc + 1] += alpha * r31;
            C[ci + 4*ldc + 0] += alpha * r40;  C[ci + 4*ldc + 1] += alpha * r41;
            C[ci + 5*ldc + 0] += alpha * r50;  C[ci + 5*ldc + 1] += alpha * r51;
            C[ci + 6*ldc + 0] += alpha * r60;  C[ci + 6*ldc + 1] += alpha * r61;
            C[ci + 7*ldc + 0] += alpha * r70;  C[ci + 7*ldc + 1] += alpha * r71;
            m_top += 2;
        }

        if (M & 1) {
            float r0=0,r1=0,r2=0,r3=0,r4=0,r5=0,r6=0,r7=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            PRAGMA_UNROLL_8
            for (BLASLONG k=0; k<K; k++) {
                float a0 = A[ai];
                r0 += a0 * B[bi+0]; r1 += a0 * B[bi+1];
                r2 += a0 * B[bi+2]; r3 += a0 * B[bi+3];
                r4 += a0 * B[bi+4]; r5 += a0 * B[bi+5];
                r6 += a0 * B[bi+6]; r7 += a0 * B[bi+7];
                ai += 1;
                bi += 8;
            }

            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;
            C[ci + 1*ldc + 0] += alpha * r1;
            C[ci + 2*ldc + 0] += alpha * r2;
            C[ci + 3*ldc + 0] += alpha * r3;
            C[ci + 4*ldc + 0] += alpha * r4;
            C[ci + 5*ldc + 0] += alpha * r5;
            C[ci + 6*ldc + 0] += alpha * r6;
            C[ci + 7*ldc + 0] += alpha * r7;
            m_top += 1;
        }
        n_top += 8;
    }

    // =========================================================
    // -- N tail: 4
    // =========================================================
    if (N & 4) {
        m_top = 0;

        for (BLASLONG i = 0; i < M/16; i += 1) {
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1], B2 = B[bi+2], B3 = B[bi+3];
            bi += 4;

            vfloat32m2_t A0 = __riscv_vle32_v_f32m2(&A[ai], gvl16);
            ai += 16;

            vfloat32m2_t r0 = __riscv_vfmul_vf_f32m2(A0, B0, gvl16);
            vfloat32m2_t r1 = __riscv_vfmul_vf_f32m2(A0, B1, gvl16);
            vfloat32m2_t r2 = __riscv_vfmul_vf_f32m2(A0, B2, gvl16);
            vfloat32m2_t r3 = __riscv_vfmul_vf_f32m2(A0, B3, gvl16);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                A0 = __riscv_vle32_v_f32m2(&A[ai], gvl16);
                ai += 16;

                r0 = __riscv_vfmacc_vf_f32m2(r0, B0, A0, gvl16);
                r1 = __riscv_vfmacc_vf_f32m2(r1, B1, A0, gvl16);
                r2 = __riscv_vfmacc_vf_f32m2(r2, B2, A0, gvl16);
                r3 = __riscv_vfmacc_vf_f32m2(r3, B3, A0, gvl16);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m2_t c0 = __riscv_vle32_v_f32m2(&C[ci + 0*ldc], gvl16);
            vfloat32m2_t c1 = __riscv_vle32_v_f32m2(&C[ci + 1*ldc], gvl16);
            vfloat32m2_t c2 = __riscv_vle32_v_f32m2(&C[ci + 2*ldc], gvl16);
            vfloat32m2_t c3 = __riscv_vle32_v_f32m2(&C[ci + 3*ldc], gvl16);

            c0 = __riscv_vfmacc_vf_f32m2(c0, alpha, r0, gvl16);
            c1 = __riscv_vfmacc_vf_f32m2(c1, alpha, r1, gvl16);
            c2 = __riscv_vfmacc_vf_f32m2(c2, alpha, r2, gvl16);
            c3 = __riscv_vfmacc_vf_f32m2(c3, alpha, r3, gvl16);

            __riscv_vse32_v_f32m2(&C[ci + 0*ldc], c0, gvl16);
            __riscv_vse32_v_f32m2(&C[ci + 1*ldc], c1, gvl16);
            __riscv_vse32_v_f32m2(&C[ci + 2*ldc], c2, gvl16);
            __riscv_vse32_v_f32m2(&C[ci + 3*ldc], c3, gvl16);

            m_top += 16;
        }

        if (M & 8) {
            gvl = __riscv_vsetvl_e32m1(8);
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1], B2 = B[bi+2], B3 = B[bi+3];
            bi += 4;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 8;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);
            vfloat32m1_t r2 = __riscv_vfmul_vf_f32m1(Av, B2, gvl);
            vfloat32m1_t r3 = __riscv_vfmul_vf_f32m1(Av, B3, gvl);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);
            vfloat32m1_t c2 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc], gvl);
            vfloat32m1_t c3 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);
            c2 = __riscv_vfmacc_vf_f32m1(c2, alpha, r2, gvl);
            c3 = __riscv_vfmacc_vf_f32m1(c3, alpha, r3, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc], c2, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc], c3, gvl);

            m_top += 8;
        }

        if (M & 4) {
            gvl = __riscv_vsetvl_e32m1(4);
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1], B2 = B[bi+2], B3 = B[bi+3];
            bi += 4;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 4;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);
            vfloat32m1_t r2 = __riscv_vfmul_vf_f32m1(Av, B2, gvl);
            vfloat32m1_t r3 = __riscv_vfmul_vf_f32m1(Av, B3, gvl);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);
            vfloat32m1_t c2 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc], gvl);
            vfloat32m1_t c3 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);
            c2 = __riscv_vfmacc_vf_f32m1(c2, alpha, r2, gvl);
            c3 = __riscv_vfmacc_vf_f32m1(c3, alpha, r3, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc], c2, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc], c3, gvl);

            m_top += 4;
        }

        if (M & 2) {
            float r00=0,r01=0, r10=0,r11=0, r20=0,r21=0, r30=0,r31=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            PRAGMA_UNROLL_8
            for (BLASLONG k=0; k<K; k++) {
                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                r20 += a0 * B[bi+2]; r21 += a1 * B[bi+2];
                r30 += a0 * B[bi+3]; r31 += a1 * B[bi+3];
                ai += 2;
                bi += 4;
            }

            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r00;  C[ci + 0*ldc + 1] += alpha * r01;
            C[ci + 1*ldc + 0] += alpha * r10;  C[ci + 1*ldc + 1] += alpha * r11;
            C[ci + 2*ldc + 0] += alpha * r20;  C[ci + 2*ldc + 1] += alpha * r21;
            C[ci + 3*ldc + 0] += alpha * r30;  C[ci + 3*ldc + 1] += alpha * r31;

            m_top += 2;
        }

        if (M & 1) {
            float r0=0,r1=0,r2=0,r3=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            PRAGMA_UNROLL_8
            for (BLASLONG k=0; k<K; k++) {
                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                r1 += a0 * B[bi+1];
                r2 += a0 * B[bi+2];
                r3 += a0 * B[bi+3];
                ai += 1;
                bi += 4;
            }

            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;
            C[ci + 1*ldc + 0] += alpha * r1;
            C[ci + 2*ldc + 0] += alpha * r2;
            C[ci + 3*ldc + 0] += alpha * r3;

            m_top += 1;
        }

        n_top += 4;
    }

    // =========================================================
    // -- N tail: 2
    // =========================================================
    if (N & 2) {
        m_top = 0;

        for (BLASLONG i = 0; i < M/16; i += 1) {
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1];
            bi += 2;

            vfloat32m2_t A0 = __riscv_vle32_v_f32m2(&A[ai], gvl16);
            ai += 16;

            vfloat32m2_t r0 = __riscv_vfmul_vf_f32m2(A0, B0, gvl16);
            vfloat32m2_t r1 = __riscv_vfmul_vf_f32m2(A0, B1, gvl16);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                A0 = __riscv_vle32_v_f32m2(&A[ai], gvl16);
                ai += 16;

                r0 = __riscv_vfmacc_vf_f32m2(r0, B0, A0, gvl16);
                r1 = __riscv_vfmacc_vf_f32m2(r1, B1, A0, gvl16);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m2_t c0 = __riscv_vle32_v_f32m2(&C[ci + 0*ldc], gvl16);
            vfloat32m2_t c1 = __riscv_vle32_v_f32m2(&C[ci + 1*ldc], gvl16);

            c0 = __riscv_vfmacc_vf_f32m2(c0, alpha, r0, gvl16);
            c1 = __riscv_vfmacc_vf_f32m2(c1, alpha, r1, gvl16);

            __riscv_vse32_v_f32m2(&C[ci + 0*ldc], c0, gvl16);
            __riscv_vse32_v_f32m2(&C[ci + 1*ldc], c1, gvl16);

            m_top += 16;
        }

        if (M & 8) {
            gvl = __riscv_vsetvl_e32m1(8);
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1];
            bi += 2;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 8;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);

            m_top += 8;
        }

        if (M & 4) {
            gvl = __riscv_vsetvl_e32m1(4);
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1];
            bi += 2;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 4;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);

            m_top += 4;
        }

        if (M & 2) {
            float r00=0,r01=0, r10=0,r11=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            PRAGMA_UNROLL_8
            for (BLASLONG k=0; k<K; k++) {
                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                ai += 2;
                bi += 2;
            }

            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r00;  C[ci + 0*ldc + 1] += alpha * r01;
            C[ci + 1*ldc + 0] += alpha * r10;  C[ci + 1*ldc + 1] += alpha * r11;

            m_top += 2;
        }

        if (M & 1) {
            float r0=0,r1=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            PRAGMA_UNROLL_8
            for (BLASLONG k=0; k<K; k++) {
                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                r1 += a0 * B[bi+1];
                ai += 1;
                bi += 2;
            }

            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;
            C[ci + 1*ldc + 0] += alpha * r1;

            m_top += 1;
        }

        n_top += 2;
    }

    // =========================================================
    // -- N tail: 1
    // =========================================================
    if (N & 1) {
        m_top = 0;

        for (BLASLONG i = 0; i < M/16; i += 1) {
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0];
            bi += 1;

            vfloat32m2_t A0 = __riscv_vle32_v_f32m2(&A[ai], gvl16);
            ai += 16;

            vfloat32m2_t r0 = __riscv_vfmul_vf_f32m2(A0, B0, gvl16);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0];
                bi += 1;

                A0 = __riscv_vle32_v_f32m2(&A[ai], gvl16);
                ai += 16;

                r0 = __riscv_vfmacc_vf_f32m2(r0, B0, A0, gvl16);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m2_t c0 = __riscv_vle32_v_f32m2(&C[ci + 0*ldc], gvl16);

            c0 = __riscv_vfmacc_vf_f32m2(c0, alpha, r0, gvl16);

            __riscv_vse32_v_f32m2(&C[ci + 0*ldc], c0, gvl16);

            m_top += 16;
        }

        if (M & 8) {
            gvl = __riscv_vsetvl_e32m1(8);
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0];
            bi += 1;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 8;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0];
                bi += 1;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);

            m_top += 8;
        }

        if (M & 4) {
            gvl = __riscv_vsetvl_e32m1(4);
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0];
            bi += 1;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 4;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);

            PRAGMA_UNROLL_8
            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi+0];
                bi += 1;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);

            m_top += 4;
        }

        if (M & 2) {
            float r00=0,r01=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            PRAGMA_UNROLL_8
            for (BLASLONG k=0; k<K; k++) {
                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                ai += 2;
                bi += 1;
            }

            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r00;  C[ci + 0*ldc + 1] += alpha * r01;

            m_top += 2;
        }

        if (M & 1) {
            float r0=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            PRAGMA_UNROLL_8
            for (BLASLONG k=0; k<K; k++) {
                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                ai += 1;
                bi += 1;
            }

            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;

            m_top += 1;
        }

        n_top += 1;
    }

    return 0;
}
