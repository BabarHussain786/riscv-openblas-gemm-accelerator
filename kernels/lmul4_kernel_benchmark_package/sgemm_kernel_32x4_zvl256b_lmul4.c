/*
AUTOGENERATED-STYLE KERNEL (FULL, NO MISSING PARTS)
Target:
  LMUL=4
  cpu='zvl256b' (VLEN=256 bits)
  param_precision='float'

Practical register-safe design for LMUL=4:
  - Use M=32 rows per vector (VL=32 with e32m4 on VLEN=256)
  - Use N=4 columns (NOT 8) to keep accumulator register pressure safe:
      vfloat32m4_t uses 4 vector registers each
      8 accumulators would exceed v-reg budget on many cores
  - Main pass: N in blocks of 4, M in blocks of 32
  - M tails: 16, 8, 4, then scalar 2, 1
  - N tails: 2, 1 with same M structure

Memory layout expectation (same as OpenBLAS packed-kernel style):
  - A is packed so that for each k, a panel of M elements is contiguous
  - B is packed so that for each k, a panel of N elements is contiguous
  - C is column-major with leading dimension ldc:
        C[ (n_top + col)*ldc + (m_top + row) ]

Compile example:
  gcc -O3 -march=rv64gcv_zvl256b -mabi=lp64d -std=c11 -Wall -Wextra \
      sgemm_kernel_32x4_zvl256b_lmul4.c -c
*/

#include <riscv_vector.h>

typedef long  BLASLONG;
typedef float FLOAT;

#ifndef CNAME
#define CNAME sgemm_kernel_32x4_zvl256b_lmul4
#endif

int CNAME(BLASLONG M, BLASLONG N, BLASLONG K,
          FLOAT alpha, FLOAT* A, FLOAT* B, FLOAT* C, BLASLONG ldc)
{
    BLASLONG m_top = 0;
    BLASLONG n_top = 0;
    size_t   gvl   = 0;

    if (M <= 0 || N <= 0) return 0;
    if (K <= 0) return 0;

    // =========================================================
    // -- MAIN PASS: N in blocks of 4
    // =========================================================
    for (BLASLONG j = 0; j < (N / 4); j += 1) {
        m_top = 0;
        gvl = __riscv_vsetvl_e32m4(32);

        // -------------------------
        // Main M loop: M/32 blocks
        // -------------------------
        for (BLASLONG i = 0; i < (M / 32); i += 1) {
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            // k = 0
            float B0 = B[bi + 0];
            float B1 = B[bi + 1];
            float B2 = B[bi + 2];
            float B3 = B[bi + 3];
            bi += 4;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 32;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);
            vfloat32m4_t r1 = __riscv_vfmul_vf_f32m4(Av, B1, gvl);
            vfloat32m4_t r2 = __riscv_vfmul_vf_f32m4(Av, B2, gvl);
            vfloat32m4_t r3 = __riscv_vfmul_vf_f32m4(Av, B3, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0];
                B1 = B[bi + 1];
                B2 = B[bi + 2];
                B3 = B[bi + 3];
                bi += 4;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 32;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m4(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m4(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m4(r3, B3, Av, gvl);
            }

            // Write-back: 4 columns, each a vector of 32 rows
            BLASLONG ci = n_top * ldc + m_top;

            // Load/FMA/Store one column at a time to reduce live v-regs
            vfloat32m4_t c;

            c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 1 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r1, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 1 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 2 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r2, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 2 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 3 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r3, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 3 * ldc], c, gvl);

            m_top += 32;
        }

        // -------------------------
        // M tails for N=4 block: 16, 8, 4, then scalar 2, 1
        // -------------------------

        if (M & 16) {
            gvl = __riscv_vsetvl_e32m4(16);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi + 0], B1 = B[bi + 1], B2 = B[bi + 2], B3 = B[bi + 3];
            bi += 4;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 16;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);
            vfloat32m4_t r1 = __riscv_vfmul_vf_f32m4(Av, B1, gvl);
            vfloat32m4_t r2 = __riscv_vfmul_vf_f32m4(Av, B2, gvl);
            vfloat32m4_t r3 = __riscv_vfmul_vf_f32m4(Av, B3, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0]; B1 = B[bi + 1]; B2 = B[bi + 2]; B3 = B[bi + 3];
                bi += 4;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 16;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m4(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m4(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m4(r3, B3, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m4_t c;
            c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 1 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r1, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 1 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 2 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r2, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 2 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 3 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r3, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 3 * ldc], c, gvl);

            m_top += 16;
        }

        if (M & 8) {
            gvl = __riscv_vsetvl_e32m4(8);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi + 0], B1 = B[bi + 1], B2 = B[bi + 2], B3 = B[bi + 3];
            bi += 4;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 8;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);
            vfloat32m4_t r1 = __riscv_vfmul_vf_f32m4(Av, B1, gvl);
            vfloat32m4_t r2 = __riscv_vfmul_vf_f32m4(Av, B2, gvl);
            vfloat32m4_t r3 = __riscv_vfmul_vf_f32m4(Av, B3, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0]; B1 = B[bi + 1]; B2 = B[bi + 2]; B3 = B[bi + 3];
                bi += 4;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m4(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m4(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m4(r3, B3, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m4_t c;
            c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 1 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r1, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 1 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 2 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r2, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 2 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 3 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r3, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 3 * ldc], c, gvl);

            m_top += 8;
        }

        if (M & 4) {
            gvl = __riscv_vsetvl_e32m4(4);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi + 0], B1 = B[bi + 1], B2 = B[bi + 2], B3 = B[bi + 3];
            bi += 4;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 4;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);
            vfloat32m4_t r1 = __riscv_vfmul_vf_f32m4(Av, B1, gvl);
            vfloat32m4_t r2 = __riscv_vfmul_vf_f32m4(Av, B2, gvl);
            vfloat32m4_t r3 = __riscv_vfmul_vf_f32m4(Av, B3, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0]; B1 = B[bi + 1]; B2 = B[bi + 2]; B3 = B[bi + 3];
                bi += 4;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m4(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m4(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m4(r3, B3, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m4_t c;
            c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 1 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r1, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 1 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 2 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r2, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 2 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 3 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r3, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 3 * ldc], c, gvl);

            m_top += 4;
        }

        if (M & 2) {
            float r00=0,r01=0, r10=0,r11=0, r20=0,r21=0, r30=0,r31=0;

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            for (BLASLONG k = 0; k < K; k++) {
                float a0 = A[ai + 0];
                float a1 = A[ai + 1];

                r00 += a0 * B[bi + 0]; r01 += a1 * B[bi + 0];
                r10 += a0 * B[bi + 1]; r11 += a1 * B[bi + 1];
                r20 += a0 * B[bi + 2]; r21 += a1 * B[bi + 2];
                r30 += a0 * B[bi + 3]; r31 += a1 * B[bi + 3];

                ai += 2;
                bi += 4;
            }

            BLASLONG ci = n_top * ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r00;  C[ci + 0*ldc + 1] += alpha * r01;
            C[ci + 1*ldc + 0] += alpha * r10;  C[ci + 1*ldc + 1] += alpha * r11;
            C[ci + 2*ldc + 0] += alpha * r20;  C[ci + 2*ldc + 1] += alpha * r21;
            C[ci + 3*ldc + 0] += alpha * r30;  C[ci + 3*ldc + 1] += alpha * r31;

            m_top += 2;
        }

        if (M & 1) {
            float r0=0,r1=0,r2=0,r3=0;

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            for (BLASLONG k = 0; k < K; k++) {
                float a0 = A[ai];

                r0 += a0 * B[bi + 0];
                r1 += a0 * B[bi + 1];
                r2 += a0 * B[bi + 2];
                r3 += a0 * B[bi + 3];

                ai += 1;
                bi += 4;
            }

            BLASLONG ci = n_top * ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;
            C[ci + 1*ldc + 0] += alpha * r1;
            C[ci + 2*ldc + 0] += alpha * r2;
            C[ci + 3*ldc + 0] += alpha * r3;

            m_top += 1;
        }

        n_top += 4;
    }

    // =========================================================
    // -- N tail: 2
    // =========================================================
    if (N & 2) {
        m_top = 0;
        gvl = __riscv_vsetvl_e32m4(32);

        for (BLASLONG i = 0; i < (M / 32); i += 1) {
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi + 0], B1 = B[bi + 1];
            bi += 2;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 32;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);
            vfloat32m4_t r1 = __riscv_vfmul_vf_f32m4(Av, B1, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0]; B1 = B[bi + 1];
                bi += 2;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 32;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m4(r1, B1, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m4_t c;
            c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 1 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r1, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 1 * ldc], c, gvl);

            m_top += 32;
        }

        // tails 16,8,4 vectors + scalar 2,1
        if (M & 16) {
            gvl = __riscv_vsetvl_e32m4(16);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi + 0], B1 = B[bi + 1];
            bi += 2;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 16;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);
            vfloat32m4_t r1 = __riscv_vfmul_vf_f32m4(Av, B1, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0]; B1 = B[bi + 1];
                bi += 2;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 16;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m4(r1, B1, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m4_t c;

            c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 1 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r1, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 1 * ldc], c, gvl);

            m_top += 16;
        }

        if (M & 8) {
            gvl = __riscv_vsetvl_e32m4(8);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi + 0], B1 = B[bi + 1];
            bi += 2;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 8;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);
            vfloat32m4_t r1 = __riscv_vfmul_vf_f32m4(Av, B1, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0]; B1 = B[bi + 1];
                bi += 2;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m4(r1, B1, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m4_t c;

            c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 1 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r1, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 1 * ldc], c, gvl);

            m_top += 8;
        }

        if (M & 4) {
            gvl = __riscv_vsetvl_e32m4(4);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi + 0], B1 = B[bi + 1];
            bi += 2;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 4;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);
            vfloat32m4_t r1 = __riscv_vfmul_vf_f32m4(Av, B1, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0]; B1 = B[bi + 1];
                bi += 2;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m4(r1, B1, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m4_t c;

            c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            c = __riscv_vle32_v_f32m4(&C[ci + 1 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r1, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 1 * ldc], c, gvl);

            m_top += 4;
        }

        if (M & 2) {
            float r00=0,r01=0, r10=0,r11=0;

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            for (BLASLONG k = 0; k < K; k++) {
                float a0 = A[ai + 0];
                float a1 = A[ai + 1];

                r00 += a0 * B[bi + 0]; r01 += a1 * B[bi + 0];
                r10 += a0 * B[bi + 1]; r11 += a1 * B[bi + 1];

                ai += 2;
                bi += 2;
            }

            BLASLONG ci = n_top * ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r00;  C[ci + 0*ldc + 1] += alpha * r01;
            C[ci + 1*ldc + 0] += alpha * r10;  C[ci + 1*ldc + 1] += alpha * r11;

            m_top += 2;
        }

        if (M & 1) {
            float r0=0,r1=0;

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            for (BLASLONG k = 0; k < K; k++) {
                float a0 = A[ai];
                r0 += a0 * B[bi + 0];
                r1 += a0 * B[bi + 1];
                ai += 1;
                bi += 2;
            }

            BLASLONG ci = n_top * ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;
            C[ci + 1*ldc + 0] += alpha * r1;

            m_top += 1;
        }

        n_top += 2;
    }

    // =========================================================
    // -- N tail: 1
    // =========================================================
    if (N & 1) {
        m_top = 0;
        gvl = __riscv_vsetvl_e32m4(32);

        for (BLASLONG i = 0; i < (M / 32); i += 1) {
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi + 0];
            bi += 1;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 32;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0];
                bi += 1;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 32;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m4_t c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            m_top += 32;
        }

        // tails 16,8,4 + scalar 2,1
        if (M & 16) {
            gvl = __riscv_vsetvl_e32m4(16);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi + 0];
            bi += 1;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 16;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0];
                bi += 1;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 16;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m4_t c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            m_top += 16;
        }

        if (M & 8) {
            gvl = __riscv_vsetvl_e32m4(8);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi + 0];
            bi += 1;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 8;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0];
                bi += 1;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m4_t c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            m_top += 8;
        }

        if (M & 4) {
            gvl = __riscv_vsetvl_e32m4(4);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi + 0];
            bi += 1;

            vfloat32m4_t Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
            ai += 4;

            vfloat32m4_t r0 = __riscv_vfmul_vf_f32m4(Av, B0, gvl);

            for (BLASLONG k = 1; k < K; k++) {
                B0 = B[bi + 0];
                bi += 1;

                Av = __riscv_vle32_v_f32m4(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m4(r0, B0, Av, gvl);
            }

            BLASLONG ci = n_top * ldc + m_top;
            vfloat32m4_t c = __riscv_vle32_v_f32m4(&C[ci + 0 * ldc], gvl);
            c = __riscv_vfmacc_vf_f32m4(c, alpha, r0, gvl);
            __riscv_vse32_v_f32m4(&C[ci + 0 * ldc], c, gvl);

            m_top += 4;
        }

        if (M & 2) {
            float r0=0,r1=0;

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            for (BLASLONG k = 0; k < K; k++) {
                float a0 = A[ai + 0];
                float a1 = A[ai + 1];
                r0 += a0 * B[bi + 0];
                r1 += a1 * B[bi + 0];
                ai += 2;
                bi += 1;
            }

            BLASLONG ci = n_top * ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;
            C[ci + 0*ldc + 1] += alpha * r1;

            m_top += 2;
        }

        if (M & 1) {
            float r0=0;

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            for (BLASLONG k = 0; k < K; k++) {
                float a0 = A[ai];
                r0 += a0 * B[bi + 0];
                ai += 1;
                bi += 1;
            }

            BLASLONG ci = n_top * ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;

            m_top += 1;
        }

        n_top += 1;
    }

    return 0;
}
