
/*
AUTOGENERATED-STYLE KERNEL (FULL, NO MISSING PARTS)
Target:
  LMUL=1
  M=16 (main)
  N=8 (main)
  cpu='zvl256b' (VLEN=256 bits)
  param_precision='float'

Structure (same idea as OpenBLAS-style RVV micro-kernel):
  - Main pass: for j in N/8, for i in M/16
  - Pointer increments:
      ai += 16 / 8 / 4   (depends on M-block)
      bi += 8 / 4 / 2 / 1 (depends on N-panel)
  - M tails: 8, 4, then scalar 2 and 1
  - N tails: 4, 2, 1 each with same M structure

LMUL=1 notes:
  - Uses vfloat32m1_t, *_f32m1 intrinsics, vsetvl_e32m1
  - With VLEN=256 and SEW=32, m1 holds up to VL=8 elements.
  - Main M=16 uses TWO vectors (8+8) to cover 16 rows.
*/

#include <riscv_vector.h>

typedef long  BLASLONG;
typedef float FLOAT;

#ifndef CNAME
#define CNAME sgemm_kernel_16x8_zvl256b_lmul1_kunroll2
#endif

int CNAME(BLASLONG M, BLASLONG N, BLASLONG K,
          FLOAT alpha, FLOAT* A, FLOAT* B, FLOAT* C, BLASLONG ldc)
{
    BLASLONG m_top = 0;
    BLASLONG n_top = 0;
    size_t   gvl8  = 0;   // VL=8 for m1 on VLEN=256
    size_t   gvl   = 0;   // for tails

    if (M <= 0 || N <= 0) return 0;
    if (K <= 0) return 0;

    gvl8 = __riscv_vsetvl_e32m1(8);

    // =========================================================
    // -- MAIN PASS: N in blocks of 8
    // =========================================================
    for (BLASLONG j = 0; j < N/8; j += 1) {
        m_top = 0;

        // -------------------------
        // Main M loop: M/16 blocks
        // -------------------------
        for (BLASLONG i = 0; i < M/16; i += 1) {
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            // k = 0
            float B0 = B[bi+0];
            float B1 = B[bi+1];
            float B2 = B[bi+2];
            float B3 = B[bi+3];
            float B4 = B[bi+4];
            float B5 = B[bi+5];
            float B6 = B[bi+6];
            float B7 = B[bi+7];
            bi += 8;

            // Load A: 16 rows as two vectors of 8
            vfloat32m1_t A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
            vfloat32m1_t A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
            ai += 16;

            // Accumulators for 8 columns, 2 vectors each (top/bottom 8 rows)
            vfloat32m1_t r0_0 = __riscv_vfmul_vf_f32m1(A0, B0, gvl8);
            vfloat32m1_t r0_1 = __riscv_vfmul_vf_f32m1(A1, B0, gvl8);

            vfloat32m1_t r1_0 = __riscv_vfmul_vf_f32m1(A0, B1, gvl8);
            vfloat32m1_t r1_1 = __riscv_vfmul_vf_f32m1(A1, B1, gvl8);

            vfloat32m1_t r2_0 = __riscv_vfmul_vf_f32m1(A0, B2, gvl8);
            vfloat32m1_t r2_1 = __riscv_vfmul_vf_f32m1(A1, B2, gvl8);

            vfloat32m1_t r3_0 = __riscv_vfmul_vf_f32m1(A0, B3, gvl8);
            vfloat32m1_t r3_1 = __riscv_vfmul_vf_f32m1(A1, B3, gvl8);

            vfloat32m1_t r4_0 = __riscv_vfmul_vf_f32m1(A0, B4, gvl8);
            vfloat32m1_t r4_1 = __riscv_vfmul_vf_f32m1(A1, B4, gvl8);

            vfloat32m1_t r5_0 = __riscv_vfmul_vf_f32m1(A0, B5, gvl8);
            vfloat32m1_t r5_1 = __riscv_vfmul_vf_f32m1(A1, B5, gvl8);

            vfloat32m1_t r6_0 = __riscv_vfmul_vf_f32m1(A0, B6, gvl8);
            vfloat32m1_t r6_1 = __riscv_vfmul_vf_f32m1(A1, B6, gvl8);

            vfloat32m1_t r7_0 = __riscv_vfmul_vf_f32m1(A0, B7, gvl8);
            vfloat32m1_t r7_1 = __riscv_vfmul_vf_f32m1(A1, B7, gvl8);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0];
                B1 = B[bi+1];
                B2 = B[bi+2];
                B3 = B[bi+3];
                B4 = B[bi+4];
                B5 = B[bi+5];
                B6 = B[bi+6];
                B7 = B[bi+7];
                bi += 8;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);

                r1_0 = __riscv_vfmacc_vf_f32m1(r1_0, B1, A0, gvl8);
                r1_1 = __riscv_vfmacc_vf_f32m1(r1_1, B1, A1, gvl8);

                r2_0 = __riscv_vfmacc_vf_f32m1(r2_0, B2, A0, gvl8);
                r2_1 = __riscv_vfmacc_vf_f32m1(r2_1, B2, A1, gvl8);

                r3_0 = __riscv_vfmacc_vf_f32m1(r3_0, B3, A0, gvl8);
                r3_1 = __riscv_vfmacc_vf_f32m1(r3_1, B3, A1, gvl8);

                r4_0 = __riscv_vfmacc_vf_f32m1(r4_0, B4, A0, gvl8);
                r4_1 = __riscv_vfmacc_vf_f32m1(r4_1, B4, A1, gvl8);

                r5_0 = __riscv_vfmacc_vf_f32m1(r5_0, B5, A0, gvl8);
                r5_1 = __riscv_vfmacc_vf_f32m1(r5_1, B5, A1, gvl8);

                r6_0 = __riscv_vfmacc_vf_f32m1(r6_0, B6, A0, gvl8);
                r6_1 = __riscv_vfmacc_vf_f32m1(r6_1, B6, A1, gvl8);

                r7_0 = __riscv_vfmacc_vf_f32m1(r7_0, B7, A0, gvl8);
                r7_1 = __riscv_vfmacc_vf_f32m1(r7_1, B7, A1, gvl8);
                            // unroll step (k+1)

                B0 = B[bi+0];
                B1 = B[bi+1];
                B2 = B[bi+2];
                B3 = B[bi+3];
                B4 = B[bi+4];
                B5 = B[bi+5];
                B6 = B[bi+6];
                B7 = B[bi+7];
                bi += 8;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);

                r1_0 = __riscv_vfmacc_vf_f32m1(r1_0, B1, A0, gvl8);
                r1_1 = __riscv_vfmacc_vf_f32m1(r1_1, B1, A1, gvl8);

                r2_0 = __riscv_vfmacc_vf_f32m1(r2_0, B2, A0, gvl8);
                r2_1 = __riscv_vfmacc_vf_f32m1(r2_1, B2, A1, gvl8);

                r3_0 = __riscv_vfmacc_vf_f32m1(r3_0, B3, A0, gvl8);
                r3_1 = __riscv_vfmacc_vf_f32m1(r3_1, B3, A1, gvl8);

                r4_0 = __riscv_vfmacc_vf_f32m1(r4_0, B4, A0, gvl8);
                r4_1 = __riscv_vfmacc_vf_f32m1(r4_1, B4, A1, gvl8);

                r5_0 = __riscv_vfmacc_vf_f32m1(r5_0, B5, A0, gvl8);
                r5_1 = __riscv_vfmacc_vf_f32m1(r5_1, B5, A1, gvl8);

                r6_0 = __riscv_vfmacc_vf_f32m1(r6_0, B6, A0, gvl8);
                r6_1 = __riscv_vfmacc_vf_f32m1(r6_1, B6, A1, gvl8);

                r7_0 = __riscv_vfmacc_vf_f32m1(r7_0, B7, A0, gvl8);
                r7_1 = __riscv_vfmacc_vf_f32m1(r7_1, B7, A1, gvl8);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0];
                B1 = B[bi+1];
                B2 = B[bi+2];
                B3 = B[bi+3];
                B4 = B[bi+4];
                B5 = B[bi+5];
                B6 = B[bi+6];
                B7 = B[bi+7];
                bi += 8;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);

                r1_0 = __riscv_vfmacc_vf_f32m1(r1_0, B1, A0, gvl8);
                r1_1 = __riscv_vfmacc_vf_f32m1(r1_1, B1, A1, gvl8);

                r2_0 = __riscv_vfmacc_vf_f32m1(r2_0, B2, A0, gvl8);
                r2_1 = __riscv_vfmacc_vf_f32m1(r2_1, B2, A1, gvl8);

                r3_0 = __riscv_vfmacc_vf_f32m1(r3_0, B3, A0, gvl8);
                r3_1 = __riscv_vfmacc_vf_f32m1(r3_1, B3, A1, gvl8);

                r4_0 = __riscv_vfmacc_vf_f32m1(r4_0, B4, A0, gvl8);
                r4_1 = __riscv_vfmacc_vf_f32m1(r4_1, B4, A1, gvl8);

                r5_0 = __riscv_vfmacc_vf_f32m1(r5_0, B5, A0, gvl8);
                r5_1 = __riscv_vfmacc_vf_f32m1(r5_1, B5, A1, gvl8);

                r6_0 = __riscv_vfmacc_vf_f32m1(r6_0, B6, A0, gvl8);
                r6_1 = __riscv_vfmacc_vf_f32m1(r6_1, B6, A1, gvl8);

                r7_0 = __riscv_vfmacc_vf_f32m1(r7_0, B7, A0, gvl8);
                r7_1 = __riscv_vfmacc_vf_f32m1(r7_1, B7, A1, gvl8);
                        }


            // Write-back: 8 columns, each has two vectors (top/bottom 8 rows)
            BLASLONG ci = n_top * ldc + m_top;

            // Column 0
            vfloat32m1_t c0_0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc + 0], gvl8);
            vfloat32m1_t c0_1 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc + 8], gvl8);
            c0_0 = __riscv_vfmacc_vf_f32m1(c0_0, alpha, r0_0, gvl8);
            c0_1 = __riscv_vfmacc_vf_f32m1(c0_1, alpha, r0_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 0*ldc + 0], c0_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 0*ldc + 8], c0_1, gvl8);

            // Column 1
            vfloat32m1_t c1_0 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc + 0], gvl8);
            vfloat32m1_t c1_1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc + 8], gvl8);
            c1_0 = __riscv_vfmacc_vf_f32m1(c1_0, alpha, r1_0, gvl8);
            c1_1 = __riscv_vfmacc_vf_f32m1(c1_1, alpha, r1_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc + 0], c1_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc + 8], c1_1, gvl8);

            // Column 2
            vfloat32m1_t c2_0 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc + 0], gvl8);
            vfloat32m1_t c2_1 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc + 8], gvl8);
            c2_0 = __riscv_vfmacc_vf_f32m1(c2_0, alpha, r2_0, gvl8);
            c2_1 = __riscv_vfmacc_vf_f32m1(c2_1, alpha, r2_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc + 0], c2_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc + 8], c2_1, gvl8);

            // Column 3
            vfloat32m1_t c3_0 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc + 0], gvl8);
            vfloat32m1_t c3_1 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc + 8], gvl8);
            c3_0 = __riscv_vfmacc_vf_f32m1(c3_0, alpha, r3_0, gvl8);
            c3_1 = __riscv_vfmacc_vf_f32m1(c3_1, alpha, r3_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc + 0], c3_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc + 8], c3_1, gvl8);

            // Column 4
            vfloat32m1_t c4_0 = __riscv_vle32_v_f32m1(&C[ci + 4*ldc + 0], gvl8);
            vfloat32m1_t c4_1 = __riscv_vle32_v_f32m1(&C[ci + 4*ldc + 8], gvl8);
            c4_0 = __riscv_vfmacc_vf_f32m1(c4_0, alpha, r4_0, gvl8);
            c4_1 = __riscv_vfmacc_vf_f32m1(c4_1, alpha, r4_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 4*ldc + 0], c4_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 4*ldc + 8], c4_1, gvl8);

            // Column 5
            vfloat32m1_t c5_0 = __riscv_vle32_v_f32m1(&C[ci + 5*ldc + 0], gvl8);
            vfloat32m1_t c5_1 = __riscv_vle32_v_f32m1(&C[ci + 5*ldc + 8], gvl8);
            c5_0 = __riscv_vfmacc_vf_f32m1(c5_0, alpha, r5_0, gvl8);
            c5_1 = __riscv_vfmacc_vf_f32m1(c5_1, alpha, r5_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 5*ldc + 0], c5_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 5*ldc + 8], c5_1, gvl8);

            // Column 6
            vfloat32m1_t c6_0 = __riscv_vle32_v_f32m1(&C[ci + 6*ldc + 0], gvl8);
            vfloat32m1_t c6_1 = __riscv_vle32_v_f32m1(&C[ci + 6*ldc + 8], gvl8);
            c6_0 = __riscv_vfmacc_vf_f32m1(c6_0, alpha, r6_0, gvl8);
            c6_1 = __riscv_vfmacc_vf_f32m1(c6_1, alpha, r6_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 6*ldc + 0], c6_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 6*ldc + 8], c6_1, gvl8);

            // Column 7
            vfloat32m1_t c7_0 = __riscv_vle32_v_f32m1(&C[ci + 7*ldc + 0], gvl8);
            vfloat32m1_t c7_1 = __riscv_vle32_v_f32m1(&C[ci + 7*ldc + 8], gvl8);
            c7_0 = __riscv_vfmacc_vf_f32m1(c7_0, alpha, r7_0, gvl8);
            c7_1 = __riscv_vfmacc_vf_f32m1(c7_1, alpha, r7_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 7*ldc + 0], c7_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 7*ldc + 8], c7_1, gvl8);

            m_top += 16;
        }

        // -------------------------
        // M tails for N=8 block: 8, 4, then scalar 2, 1
        // -------------------------

        if (M & 8) {
            gvl = __riscv_vsetvl_e32m1(8);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1], B2 = B[bi+2], B3 = B[bi+3];
            float B4 = B[bi+4], B5 = B[bi+5], B6 = B[bi+6], B7 = B[bi+7];
            bi += 8;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 8;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);
            vfloat32m1_t r2 = __riscv_vfmul_vf_f32m1(Av, B2, gvl);
            vfloat32m1_t r3 = __riscv_vfmul_vf_f32m1(Av, B3, gvl);
            vfloat32m1_t r4 = __riscv_vfmul_vf_f32m1(Av, B4, gvl);
            vfloat32m1_t r5 = __riscv_vfmul_vf_f32m1(Av, B5, gvl);
            vfloat32m1_t r6 = __riscv_vfmul_vf_f32m1(Av, B6, gvl);
            vfloat32m1_t r7 = __riscv_vfmul_vf_f32m1(Av, B7, gvl);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                B4 = B[bi+4]; B5 = B[bi+5]; B6 = B[bi+6]; B7 = B[bi+7];
                bi += 8;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                r4 = __riscv_vfmacc_vf_f32m1(r4, B4, Av, gvl);
                r5 = __riscv_vfmacc_vf_f32m1(r5, B5, Av, gvl);
                r6 = __riscv_vfmacc_vf_f32m1(r6, B6, Av, gvl);
                r7 = __riscv_vfmacc_vf_f32m1(r7, B7, Av, gvl);
                            // unroll step (k+1)

                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                B4 = B[bi+4]; B5 = B[bi+5]; B6 = B[bi+6]; B7 = B[bi+7];
                bi += 8;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                r4 = __riscv_vfmacc_vf_f32m1(r4, B4, Av, gvl);
                r5 = __riscv_vfmacc_vf_f32m1(r5, B5, Av, gvl);
                r6 = __riscv_vfmacc_vf_f32m1(r6, B6, Av, gvl);
                r7 = __riscv_vfmacc_vf_f32m1(r7, B7, Av, gvl);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                B4 = B[bi+4]; B5 = B[bi+5]; B6 = B[bi+6]; B7 = B[bi+7];
                bi += 8;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                r4 = __riscv_vfmacc_vf_f32m1(r4, B4, Av, gvl);
                r5 = __riscv_vfmacc_vf_f32m1(r5, B5, Av, gvl);
                r6 = __riscv_vfmacc_vf_f32m1(r6, B6, Av, gvl);
                r7 = __riscv_vfmacc_vf_f32m1(r7, B7, Av, gvl);
                        }


            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);
            vfloat32m1_t c2 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc], gvl);
            vfloat32m1_t c3 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc], gvl);
            vfloat32m1_t c4 = __riscv_vle32_v_f32m1(&C[ci + 4*ldc], gvl);
            vfloat32m1_t c5 = __riscv_vle32_v_f32m1(&C[ci + 5*ldc], gvl);
            vfloat32m1_t c6 = __riscv_vle32_v_f32m1(&C[ci + 6*ldc], gvl);
            vfloat32m1_t c7 = __riscv_vle32_v_f32m1(&C[ci + 7*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);
            c2 = __riscv_vfmacc_vf_f32m1(c2, alpha, r2, gvl);
            c3 = __riscv_vfmacc_vf_f32m1(c3, alpha, r3, gvl);
            c4 = __riscv_vfmacc_vf_f32m1(c4, alpha, r4, gvl);
            c5 = __riscv_vfmacc_vf_f32m1(c5, alpha, r5, gvl);
            c6 = __riscv_vfmacc_vf_f32m1(c6, alpha, r6, gvl);
            c7 = __riscv_vfmacc_vf_f32m1(c7, alpha, r7, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc], c2, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc], c3, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 4*ldc], c4, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 5*ldc], c5, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 6*ldc], c6, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 7*ldc], c7, gvl);

            m_top += 8;
        }

        if (M & 4) {
            gvl = __riscv_vsetvl_e32m1(4);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1], B2 = B[bi+2], B3 = B[bi+3];
            float B4 = B[bi+4], B5 = B[bi+5], B6 = B[bi+6], B7 = B[bi+7];
            bi += 8;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 4;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);
            vfloat32m1_t r2 = __riscv_vfmul_vf_f32m1(Av, B2, gvl);
            vfloat32m1_t r3 = __riscv_vfmul_vf_f32m1(Av, B3, gvl);
            vfloat32m1_t r4 = __riscv_vfmul_vf_f32m1(Av, B4, gvl);
            vfloat32m1_t r5 = __riscv_vfmul_vf_f32m1(Av, B5, gvl);
            vfloat32m1_t r6 = __riscv_vfmul_vf_f32m1(Av, B6, gvl);
            vfloat32m1_t r7 = __riscv_vfmul_vf_f32m1(Av, B7, gvl);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                B4 = B[bi+4]; B5 = B[bi+5]; B6 = B[bi+6]; B7 = B[bi+7];
                bi += 8;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                r4 = __riscv_vfmacc_vf_f32m1(r4, B4, Av, gvl);
                r5 = __riscv_vfmacc_vf_f32m1(r5, B5, Av, gvl);
                r6 = __riscv_vfmacc_vf_f32m1(r6, B6, Av, gvl);
                r7 = __riscv_vfmacc_vf_f32m1(r7, B7, Av, gvl);
                            // unroll step (k+1)

                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                B4 = B[bi+4]; B5 = B[bi+5]; B6 = B[bi+6]; B7 = B[bi+7];
                bi += 8;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                r4 = __riscv_vfmacc_vf_f32m1(r4, B4, Av, gvl);
                r5 = __riscv_vfmacc_vf_f32m1(r5, B5, Av, gvl);
                r6 = __riscv_vfmacc_vf_f32m1(r6, B6, Av, gvl);
                r7 = __riscv_vfmacc_vf_f32m1(r7, B7, Av, gvl);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                B4 = B[bi+4]; B5 = B[bi+5]; B6 = B[bi+6]; B7 = B[bi+7];
                bi += 8;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                r4 = __riscv_vfmacc_vf_f32m1(r4, B4, Av, gvl);
                r5 = __riscv_vfmacc_vf_f32m1(r5, B5, Av, gvl);
                r6 = __riscv_vfmacc_vf_f32m1(r6, B6, Av, gvl);
                r7 = __riscv_vfmacc_vf_f32m1(r7, B7, Av, gvl);
                        }


            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);
            vfloat32m1_t c2 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc], gvl);
            vfloat32m1_t c3 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc], gvl);
            vfloat32m1_t c4 = __riscv_vle32_v_f32m1(&C[ci + 4*ldc], gvl);
            vfloat32m1_t c5 = __riscv_vle32_v_f32m1(&C[ci + 5*ldc], gvl);
            vfloat32m1_t c6 = __riscv_vle32_v_f32m1(&C[ci + 6*ldc], gvl);
            vfloat32m1_t c7 = __riscv_vle32_v_f32m1(&C[ci + 7*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);
            c2 = __riscv_vfmacc_vf_f32m1(c2, alpha, r2, gvl);
            c3 = __riscv_vfmacc_vf_f32m1(c3, alpha, r3, gvl);
            c4 = __riscv_vfmacc_vf_f32m1(c4, alpha, r4, gvl);
            c5 = __riscv_vfmacc_vf_f32m1(c5, alpha, r5, gvl);
            c6 = __riscv_vfmacc_vf_f32m1(c6, alpha, r6, gvl);
            c7 = __riscv_vfmacc_vf_f32m1(c7, alpha, r7, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc], c2, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc], c3, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 4*ldc], c4, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 5*ldc], c5, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 6*ldc], c6, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 7*ldc], c7, gvl);

            m_top += 4;
        }

        if (M & 2) {
            float r00=0,r01=0, r10=0,r11=0, r20=0,r21=0, r30=0,r31=0;
            float r40=0,r41=0, r50=0,r51=0, r60=0,r61=0, r70=0,r71=0;

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            BLASLONG k = 0;
            for (; k + 1 < K; k += 2) {

                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                r20 += a0 * B[bi+2]; r21 += a1 * B[bi+2];
                r30 += a0 * B[bi+3]; r31 += a1 * B[bi+3];
                r40 += a0 * B[bi+4]; r41 += a1 * B[bi+4];
                r50 += a0 * B[bi+5]; r51 += a1 * B[bi+5];
                r60 += a0 * B[bi+6]; r61 += a1 * B[bi+6];
                r70 += a0 * B[bi+7]; r71 += a1 * B[bi+7];
                ai += 2;
                bi += 8;
                            // unroll step (k+1)

                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                r20 += a0 * B[bi+2]; r21 += a1 * B[bi+2];
                r30 += a0 * B[bi+3]; r31 += a1 * B[bi+3];
                r40 += a0 * B[bi+4]; r41 += a1 * B[bi+4];
                r50 += a0 * B[bi+5]; r51 += a1 * B[bi+5];
                r60 += a0 * B[bi+6]; r61 += a1 * B[bi+6];
                r70 += a0 * B[bi+7]; r71 += a1 * B[bi+7];
                ai += 2;
                bi += 8;
                        }
            for (; k < K; k++) {
                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                r20 += a0 * B[bi+2]; r21 += a1 * B[bi+2];
                r30 += a0 * B[bi+3]; r31 += a1 * B[bi+3];
                r40 += a0 * B[bi+4]; r41 += a1 * B[bi+4];
                r50 += a0 * B[bi+5]; r51 += a1 * B[bi+5];
                r60 += a0 * B[bi+6]; r61 += a1 * B[bi+6];
                r70 += a0 * B[bi+7]; r71 += a1 * B[bi+7];
                ai += 2;
                bi += 8;
                        }


            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r00;  C[ci + 0*ldc + 1] += alpha * r01;
            C[ci + 1*ldc + 0] += alpha * r10;  C[ci + 1*ldc + 1] += alpha * r11;
            C[ci + 2*ldc + 0] += alpha * r20;  C[ci + 2*ldc + 1] += alpha * r21;
            C[ci + 3*ldc + 0] += alpha * r30;  C[ci + 3*ldc + 1] += alpha * r31;
            C[ci + 4*ldc + 0] += alpha * r40;  C[ci + 4*ldc + 1] += alpha * r41;
            C[ci + 5*ldc + 0] += alpha * r50;  C[ci + 5*ldc + 1] += alpha * r51;
            C[ci + 6*ldc + 0] += alpha * r60;  C[ci + 6*ldc + 1] += alpha * r61;
            C[ci + 7*ldc + 0] += alpha * r70;  C[ci + 7*ldc + 1] += alpha * r71;

            m_top += 2;
        }

        if (M & 1) {
            float r0=0,r1=0,r2=0,r3=0,r4=0,r5=0,r6=0,r7=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            BLASLONG k = 0;
            for (; k + 1 < K; k += 2) {

                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                r1 += a0 * B[bi+1];
                r2 += a0 * B[bi+2];
                r3 += a0 * B[bi+3];
                r4 += a0 * B[bi+4];
                r5 += a0 * B[bi+5];
                r6 += a0 * B[bi+6];
                r7 += a0 * B[bi+7];
                ai += 1;
                bi += 8;
                            // unroll step (k+1)

                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                r1 += a0 * B[bi+1];
                r2 += a0 * B[bi+2];
                r3 += a0 * B[bi+3];
                r4 += a0 * B[bi+4];
                r5 += a0 * B[bi+5];
                r6 += a0 * B[bi+6];
                r7 += a0 * B[bi+7];
                ai += 1;
                bi += 8;
                        }
            for (; k < K; k++) {
                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                r1 += a0 * B[bi+1];
                r2 += a0 * B[bi+2];
                r3 += a0 * B[bi+3];
                r4 += a0 * B[bi+4];
                r5 += a0 * B[bi+5];
                r6 += a0 * B[bi+6];
                r7 += a0 * B[bi+7];
                ai += 1;
                bi += 8;
                        }


            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;
            C[ci + 1*ldc + 0] += alpha * r1;
            C[ci + 2*ldc + 0] += alpha * r2;
            C[ci + 3*ldc + 0] += alpha * r3;
            C[ci + 4*ldc + 0] += alpha * r4;
            C[ci + 5*ldc + 0] += alpha * r5;
            C[ci + 6*ldc + 0] += alpha * r6;
            C[ci + 7*ldc + 0] += alpha * r7;

            m_top += 1;
        }

        n_top += 8;
    }

    // =========================================================
    // -- N tail: 4
    // =========================================================
    if (N & 4) {
        m_top = 0;

        for (BLASLONG i = 0; i < M/16; i += 1) {
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1], B2 = B[bi+2], B3 = B[bi+3];
            bi += 4;

            vfloat32m1_t A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
            vfloat32m1_t A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
            ai += 16;

            vfloat32m1_t r0_0 = __riscv_vfmul_vf_f32m1(A0, B0, gvl8);
            vfloat32m1_t r0_1 = __riscv_vfmul_vf_f32m1(A1, B0, gvl8);
            vfloat32m1_t r1_0 = __riscv_vfmul_vf_f32m1(A0, B1, gvl8);
            vfloat32m1_t r1_1 = __riscv_vfmul_vf_f32m1(A1, B1, gvl8);
            vfloat32m1_t r2_0 = __riscv_vfmul_vf_f32m1(A0, B2, gvl8);
            vfloat32m1_t r2_1 = __riscv_vfmul_vf_f32m1(A1, B2, gvl8);
            vfloat32m1_t r3_0 = __riscv_vfmul_vf_f32m1(A0, B3, gvl8);
            vfloat32m1_t r3_1 = __riscv_vfmul_vf_f32m1(A1, B3, gvl8);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);
                r1_0 = __riscv_vfmacc_vf_f32m1(r1_0, B1, A0, gvl8);
                r1_1 = __riscv_vfmacc_vf_f32m1(r1_1, B1, A1, gvl8);
                r2_0 = __riscv_vfmacc_vf_f32m1(r2_0, B2, A0, gvl8);
                r2_1 = __riscv_vfmacc_vf_f32m1(r2_1, B2, A1, gvl8);
                r3_0 = __riscv_vfmacc_vf_f32m1(r3_0, B3, A0, gvl8);
                r3_1 = __riscv_vfmacc_vf_f32m1(r3_1, B3, A1, gvl8);
                            // unroll step (k+1)

                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);
                r1_0 = __riscv_vfmacc_vf_f32m1(r1_0, B1, A0, gvl8);
                r1_1 = __riscv_vfmacc_vf_f32m1(r1_1, B1, A1, gvl8);
                r2_0 = __riscv_vfmacc_vf_f32m1(r2_0, B2, A0, gvl8);
                r2_1 = __riscv_vfmacc_vf_f32m1(r2_1, B2, A1, gvl8);
                r3_0 = __riscv_vfmacc_vf_f32m1(r3_0, B3, A0, gvl8);
                r3_1 = __riscv_vfmacc_vf_f32m1(r3_1, B3, A1, gvl8);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);
                r1_0 = __riscv_vfmacc_vf_f32m1(r1_0, B1, A0, gvl8);
                r1_1 = __riscv_vfmacc_vf_f32m1(r1_1, B1, A1, gvl8);
                r2_0 = __riscv_vfmacc_vf_f32m1(r2_0, B2, A0, gvl8);
                r2_1 = __riscv_vfmacc_vf_f32m1(r2_1, B2, A1, gvl8);
                r3_0 = __riscv_vfmacc_vf_f32m1(r3_0, B3, A0, gvl8);
                r3_1 = __riscv_vfmacc_vf_f32m1(r3_1, B3, A1, gvl8);
                        }


            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m1_t c0_0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc + 0], gvl8);
            vfloat32m1_t c0_1 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc + 8], gvl8);
            vfloat32m1_t c1_0 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc + 0], gvl8);
            vfloat32m1_t c1_1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc + 8], gvl8);
            vfloat32m1_t c2_0 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc + 0], gvl8);
            vfloat32m1_t c2_1 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc + 8], gvl8);
            vfloat32m1_t c3_0 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc + 0], gvl8);
            vfloat32m1_t c3_1 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc + 8], gvl8);

            c0_0 = __riscv_vfmacc_vf_f32m1(c0_0, alpha, r0_0, gvl8);
            c0_1 = __riscv_vfmacc_vf_f32m1(c0_1, alpha, r0_1, gvl8);
            c1_0 = __riscv_vfmacc_vf_f32m1(c1_0, alpha, r1_0, gvl8);
            c1_1 = __riscv_vfmacc_vf_f32m1(c1_1, alpha, r1_1, gvl8);
            c2_0 = __riscv_vfmacc_vf_f32m1(c2_0, alpha, r2_0, gvl8);
            c2_1 = __riscv_vfmacc_vf_f32m1(c2_1, alpha, r2_1, gvl8);
            c3_0 = __riscv_vfmacc_vf_f32m1(c3_0, alpha, r3_0, gvl8);
            c3_1 = __riscv_vfmacc_vf_f32m1(c3_1, alpha, r3_1, gvl8);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc + 0], c0_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 0*ldc + 8], c0_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc + 0], c1_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc + 8], c1_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc + 0], c2_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc + 8], c2_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc + 0], c3_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc + 8], c3_1, gvl8);

            m_top += 16;
        }

        // Tails for N=4: reuse the same M tail structure as above (8,4,2,1)
        // To keep this file self-contained and correct, we fall back to the LMUL=2-style tail blocks,
        // but implemented with LMUL=1 (single vector) where applicable.

        if (M & 8) {
            gvl = __riscv_vsetvl_e32m1(8);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1], B2 = B[bi+2], B3 = B[bi+3];
            bi += 4;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 8;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);
            vfloat32m1_t r2 = __riscv_vfmul_vf_f32m1(Av, B2, gvl);
            vfloat32m1_t r3 = __riscv_vfmul_vf_f32m1(Av, B3, gvl);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                            // unroll step (k+1)

                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                        }


            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);
            vfloat32m1_t c2 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc], gvl);
            vfloat32m1_t c3 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);
            c2 = __riscv_vfmacc_vf_f32m1(c2, alpha, r2, gvl);
            c3 = __riscv_vfmacc_vf_f32m1(c3, alpha, r3, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc], c2, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc], c3, gvl);

            m_top += 8;
        }

        if (M & 4) {
            gvl = __riscv_vsetvl_e32m1(4);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1], B2 = B[bi+2], B3 = B[bi+3];
            bi += 4;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 4;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);
            vfloat32m1_t r2 = __riscv_vfmul_vf_f32m1(Av, B2, gvl);
            vfloat32m1_t r3 = __riscv_vfmul_vf_f32m1(Av, B3, gvl);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                            // unroll step (k+1)

                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1]; B2 = B[bi+2]; B3 = B[bi+3];
                bi += 4;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                r2 = __riscv_vfmacc_vf_f32m1(r2, B2, Av, gvl);
                r3 = __riscv_vfmacc_vf_f32m1(r3, B3, Av, gvl);
                        }


            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);
            vfloat32m1_t c2 = __riscv_vle32_v_f32m1(&C[ci + 2*ldc], gvl);
            vfloat32m1_t c3 = __riscv_vle32_v_f32m1(&C[ci + 3*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);
            c2 = __riscv_vfmacc_vf_f32m1(c2, alpha, r2, gvl);
            c3 = __riscv_vfmacc_vf_f32m1(c3, alpha, r3, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 2*ldc], c2, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 3*ldc], c3, gvl);

            m_top += 4;
        }

        if (M & 2) {
            float r00=0,r01=0, r10=0,r11=0, r20=0,r21=0, r30=0,r31=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            BLASLONG k = 0;
            for (; k + 1 < K; k += 2) {

                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                r20 += a0 * B[bi+2]; r21 += a1 * B[bi+2];
                r30 += a0 * B[bi+3]; r31 += a1 * B[bi+3];
                ai += 2;
                bi += 4;
                            // unroll step (k+1)

                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                r20 += a0 * B[bi+2]; r21 += a1 * B[bi+2];
                r30 += a0 * B[bi+3]; r31 += a1 * B[bi+3];
                ai += 2;
                bi += 4;
                        }
            for (; k < K; k++) {
                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                r20 += a0 * B[bi+2]; r21 += a1 * B[bi+2];
                r30 += a0 * B[bi+3]; r31 += a1 * B[bi+3];
                ai += 2;
                bi += 4;
                        }


            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r00;  C[ci + 0*ldc + 1] += alpha * r01;
            C[ci + 1*ldc + 0] += alpha * r10;  C[ci + 1*ldc + 1] += alpha * r11;
            C[ci + 2*ldc + 0] += alpha * r20;  C[ci + 2*ldc + 1] += alpha * r21;
            C[ci + 3*ldc + 0] += alpha * r30;  C[ci + 3*ldc + 1] += alpha * r31;

            m_top += 2;
        }

        if (M & 1) {
            float r0=0,r1=0,r2=0,r3=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            BLASLONG k = 0;
            for (; k + 1 < K; k += 2) {

                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                r1 += a0 * B[bi+1];
                r2 += a0 * B[bi+2];
                r3 += a0 * B[bi+3];
                ai += 1;
                bi += 4;
                            // unroll step (k+1)

                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                r1 += a0 * B[bi+1];
                r2 += a0 * B[bi+2];
                r3 += a0 * B[bi+3];
                ai += 1;
                bi += 4;
                        }
            for (; k < K; k++) {
                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                r1 += a0 * B[bi+1];
                r2 += a0 * B[bi+2];
                r3 += a0 * B[bi+3];
                ai += 1;
                bi += 4;
                        }


            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;
            C[ci + 1*ldc + 0] += alpha * r1;
            C[ci + 2*ldc + 0] += alpha * r2;
            C[ci + 3*ldc + 0] += alpha * r3;

            m_top += 1;
        }

        n_top += 4;
    }

    // =========================================================
    // -- N tail: 2
    // =========================================================
    if (N & 2) {
        m_top = 0;

        for (BLASLONG i = 0; i < M/16; i += 1) {
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1];
            bi += 2;

            vfloat32m1_t A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
            vfloat32m1_t A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
            ai += 16;

            vfloat32m1_t r0_0 = __riscv_vfmul_vf_f32m1(A0, B0, gvl8);
            vfloat32m1_t r0_1 = __riscv_vfmul_vf_f32m1(A1, B0, gvl8);
            vfloat32m1_t r1_0 = __riscv_vfmul_vf_f32m1(A0, B1, gvl8);
            vfloat32m1_t r1_1 = __riscv_vfmul_vf_f32m1(A1, B1, gvl8);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);
                r1_0 = __riscv_vfmacc_vf_f32m1(r1_0, B1, A0, gvl8);
                r1_1 = __riscv_vfmacc_vf_f32m1(r1_1, B1, A1, gvl8);
                            // unroll step (k+1)

                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);
                r1_0 = __riscv_vfmacc_vf_f32m1(r1_0, B1, A0, gvl8);
                r1_1 = __riscv_vfmacc_vf_f32m1(r1_1, B1, A1, gvl8);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);
                r1_0 = __riscv_vfmacc_vf_f32m1(r1_0, B1, A0, gvl8);
                r1_1 = __riscv_vfmacc_vf_f32m1(r1_1, B1, A1, gvl8);
                        }


            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m1_t c0_0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc + 0], gvl8);
            vfloat32m1_t c0_1 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc + 8], gvl8);
            vfloat32m1_t c1_0 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc + 0], gvl8);
            vfloat32m1_t c1_1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc + 8], gvl8);

            c0_0 = __riscv_vfmacc_vf_f32m1(c0_0, alpha, r0_0, gvl8);
            c0_1 = __riscv_vfmacc_vf_f32m1(c0_1, alpha, r0_1, gvl8);
            c1_0 = __riscv_vfmacc_vf_f32m1(c1_0, alpha, r1_0, gvl8);
            c1_1 = __riscv_vfmacc_vf_f32m1(c1_1, alpha, r1_1, gvl8);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc + 0], c0_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 0*ldc + 8], c0_1, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc + 0], c1_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc + 8], c1_1, gvl8);

            m_top += 16;
        }

        // M tails for N=2: 8, 4, 2, 1 (vector/scalar)
        if (M & 8) {
            gvl = __riscv_vsetvl_e32m1(8);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1];
            bi += 2;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 8;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                            // unroll step (k+1)

                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                        }


            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);

            m_top += 8;
        }

        if (M & 4) {
            gvl = __riscv_vsetvl_e32m1(4);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0], B1 = B[bi+1];
            bi += 2;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 4;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);
            vfloat32m1_t r1 = __riscv_vfmul_vf_f32m1(Av, B1, gvl);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                            // unroll step (k+1)

                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0]; B1 = B[bi+1];
                bi += 2;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                r1 = __riscv_vfmacc_vf_f32m1(r1, B1, Av, gvl);
                        }


            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            vfloat32m1_t c1 = __riscv_vle32_v_f32m1(&C[ci + 1*ldc], gvl);

            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            c1 = __riscv_vfmacc_vf_f32m1(c1, alpha, r1, gvl);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 1*ldc], c1, gvl);

            m_top += 4;
        }

        if (M & 2) {
            float r00=0,r01=0, r10=0,r11=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            BLASLONG k = 0;
            for (; k + 1 < K; k += 2) {

                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                ai += 2;
                bi += 2;
                            // unroll step (k+1)

                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                ai += 2;
                bi += 2;
                        }
            for (; k < K; k++) {
                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r00 += a0 * B[bi+0]; r01 += a1 * B[bi+0];
                r10 += a0 * B[bi+1]; r11 += a1 * B[bi+1];
                ai += 2;
                bi += 2;
                        }


            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r00;  C[ci + 0*ldc + 1] += alpha * r01;
            C[ci + 1*ldc + 0] += alpha * r10;  C[ci + 1*ldc + 1] += alpha * r11;

            m_top += 2;
        }

        if (M & 1) {
            float r0=0,r1=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            BLASLONG k = 0;
            for (; k + 1 < K; k += 2) {

                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                r1 += a0 * B[bi+1];
                ai += 1;
                bi += 2;
                            // unroll step (k+1)

                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                r1 += a0 * B[bi+1];
                ai += 1;
                bi += 2;
                        }
            for (; k < K; k++) {
                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                r1 += a0 * B[bi+1];
                ai += 1;
                bi += 2;
                        }


            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;
            C[ci + 1*ldc + 0] += alpha * r1;

            m_top += 1;
        }

        n_top += 2;
    }

    // =========================================================
    // -- N tail: 1
    // =========================================================
    if (N & 1) {
        m_top = 0;

        for (BLASLONG i = 0; i < M/16; i += 1) {
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0];
            bi += 1;

            vfloat32m1_t A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
            vfloat32m1_t A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
            ai += 16;

            vfloat32m1_t r0_0 = __riscv_vfmul_vf_f32m1(A0, B0, gvl8);
            vfloat32m1_t r0_1 = __riscv_vfmul_vf_f32m1(A1, B0, gvl8);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0];
                bi += 1;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);
                            // unroll step (k+1)

                B0 = B[bi+0];
                bi += 1;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0];
                bi += 1;

                A0 = __riscv_vle32_v_f32m1(&A[ai + 0], gvl8);
                A1 = __riscv_vle32_v_f32m1(&A[ai + 8], gvl8);
                ai += 16;

                r0_0 = __riscv_vfmacc_vf_f32m1(r0_0, B0, A0, gvl8);
                r0_1 = __riscv_vfmacc_vf_f32m1(r0_1, B0, A1, gvl8);
                        }


            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m1_t c0_0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc + 0], gvl8);
            vfloat32m1_t c0_1 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc + 8], gvl8);

            c0_0 = __riscv_vfmacc_vf_f32m1(c0_0, alpha, r0_0, gvl8);
            c0_1 = __riscv_vfmacc_vf_f32m1(c0_1, alpha, r0_1, gvl8);

            __riscv_vse32_v_f32m1(&C[ci + 0*ldc + 0], c0_0, gvl8);
            __riscv_vse32_v_f32m1(&C[ci + 0*ldc + 8], c0_1, gvl8);

            m_top += 16;
        }

        // M tails for N=1: 8,4,2,1
        if (M & 8) {
            gvl = __riscv_vsetvl_e32m1(8);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0];
            bi += 1;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 8;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0];
                bi += 1;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                            // unroll step (k+1)

                B0 = B[bi+0];
                bi += 1;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0];
                bi += 1;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 8;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                        }


            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);

            m_top += 8;
        }

        if (M & 4) {
            gvl = __riscv_vsetvl_e32m1(4);

            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            float B0 = B[bi+0];
            bi += 1;

            vfloat32m1_t Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
            ai += 4;

            vfloat32m1_t r0 = __riscv_vfmul_vf_f32m1(Av, B0, gvl);

            BLASLONG k = 1;
            for (; k + 1 < K; k += 2) {

                B0 = B[bi+0];
                bi += 1;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                            // unroll step (k+1)

                B0 = B[bi+0];
                bi += 1;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                        }
            for (; k < K; k++) {
                B0 = B[bi+0];
                bi += 1;

                Av = __riscv_vle32_v_f32m1(&A[ai], gvl);
                ai += 4;

                r0 = __riscv_vfmacc_vf_f32m1(r0, B0, Av, gvl);
                        }


            BLASLONG ci = n_top * ldc + m_top;

            vfloat32m1_t c0 = __riscv_vle32_v_f32m1(&C[ci + 0*ldc], gvl);
            c0 = __riscv_vfmacc_vf_f32m1(c0, alpha, r0, gvl);
            __riscv_vse32_v_f32m1(&C[ci + 0*ldc], c0, gvl);

            m_top += 4;
        }

        if (M & 2) {
            float r0=0,r1=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            BLASLONG k = 0;
            for (; k + 1 < K; k += 2) {

                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r0 += a0 * B[bi+0];
                r1 += a1 * B[bi+0];
                ai += 2;
                bi += 1;
                            // unroll step (k+1)

                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r0 += a0 * B[bi+0];
                r1 += a1 * B[bi+0];
                ai += 2;
                bi += 1;
                        }
            for (; k < K; k++) {
                float a0 = A[ai+0];
                float a1 = A[ai+1];
                r0 += a0 * B[bi+0];
                r1 += a1 * B[bi+0];
                ai += 2;
                bi += 1;
                        }


            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;
            C[ci + 0*ldc + 1] += alpha * r1;

            m_top += 2;
        }

        if (M & 1) {
            float r0=0;
            BLASLONG ai = m_top * K;
            BLASLONG bi = n_top * K;

            BLASLONG k = 0;
            for (; k + 1 < K; k += 2) {

                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                ai += 1;
                bi += 1;
                            // unroll step (k+1)

                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                ai += 1;
                bi += 1;
                        }
            for (; k < K; k++) {
                float a0 = A[ai];
                r0 += a0 * B[bi+0];
                ai += 1;
                bi += 1;
                        }


            BLASLONG ci = n_top*ldc + m_top;
            C[ci + 0*ldc + 0] += alpha * r0;

            m_top += 1;
        }

        n_top += 1;
    }

    return 0;
}

/*
Compile (example on RVV toolchain):
  gcc -O3 -march=rv64gcv_zvl256b -mabi=lp64d -std=c11 -Wall -Wextra \
      sgemm_kernel_16x8_zvl256b_lmul1_kunroll2.c -c
*/
